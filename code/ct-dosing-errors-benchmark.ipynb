{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c81587-5b43-42fc-9ac2-d159980c82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPREHENSIVE LIGHTGBM PIPELINE WITH ABLATION & REPORTING\n",
    "# Complete analysis including feature importance, ablation, and visualizations\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Feature ranges: \n",
    "# - medical_patterns: 0-42 (43 features)\n",
    "# - word_char_features: 43-3062 (3020 features)\n",
    "# - sentence_embeddings: 3063-3448 (386 features)\n",
    "# - transformer_scores: 3449-3450 (2 features)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    balanced_accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "OUTPUT_DIR = \"out-final\"\n",
    "FEATURE_DIR = \"features\"\n",
    "PREFIX = \"split_tr8000_word50_char3000_ng3-7_all-MiniLM-L6-v2_FULL\"\n",
    "\n",
    "# Dataset revision (use local cache)\n",
    "DATASET_REVISION = \"ee5e7f3c00400ab56b2aa407c2d9088c9d0b01db\"\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load specific revision from local cache\n",
    "print(f\"Using dataset revision: {DATASET_REVISION}\")\n",
    "\n",
    "ds_train = load_dataset(\n",
    "    \"sssohrab/ct-dosing-errors-benchmark\", \n",
    "    split=\"train\",\n",
    "    revision=DATASET_REVISION,\n",
    "    download_mode=\"reuse_dataset_if_exists\"\n",
    ")\n",
    "ds_val = load_dataset(\n",
    "    \"sssohrab/ct-dosing-errors-benchmark\", \n",
    "    split=\"validation\",\n",
    "    revision=DATASET_REVISION,\n",
    "    download_mode=\"reuse_dataset_if_exists\"\n",
    ")\n",
    "ds_test = load_dataset(\n",
    "    \"sssohrab/ct-dosing-errors-benchmark\", \n",
    "    split=\"test\",\n",
    "    revision=DATASET_REVISION,\n",
    "    download_mode=\"reuse_dataset_if_exists\"\n",
    ")\n",
    "\n",
    "df_train = ds_train.to_pandas()\n",
    "df_val   = ds_val.to_pandas()\n",
    "df_test  = ds_test.to_pandas()\n",
    "\n",
    "y_train = df_train[\"target\"].values\n",
    "y_val   = df_val[\"target\"].values\n",
    "y_test  = df_test[\"target\"].values\n",
    "\n",
    "print(f\"Train: {len(y_train)} samples, {y_train.sum()} positive ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Val:   {len(y_val)} samples, {y_val.sum()} positive ({y_val.mean()*100:.2f}%)\")\n",
    "print(f\"Test:  {len(y_test)} samples, {y_test.sum()} positive ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = sparse.load_npz(os.path.join(FEATURE_DIR, f\"{PREFIX}_X_train.npz\"))\n",
    "X_val   = sparse.load_npz(os.path.join(FEATURE_DIR, f\"{PREFIX}_X_val.npz\"))\n",
    "X_test  = sparse.load_npz(os.path.join(FEATURE_DIR, f\"{PREFIX}_X_test.npz\"))\n",
    "\n",
    "print(f\"Train features: {X_train.shape}\")\n",
    "print(f\"Val features:   {X_val.shape}\")\n",
    "print(f\"Test features:  {X_test.shape}\")\n",
    "\n",
    "# Combine train + val for cross-validation\n",
    "X_all = sparse.vstack([X_train, X_val])\n",
    "y_all = np.concatenate([y_train, y_val])\n",
    "\n",
    "print(f\"\\nCombined for CV: {X_all.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE CATEGORY IDENTIFICATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFYING FEATURE CATEGORIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Try to infer categories or use known structure\n",
    "if n_features == 3449:\n",
    "    feature_categories = {\n",
    "        'medical_patterns': (0, 43),\n",
    "        'word_char_features': (43, 3043),\n",
    "        'sentence_embeddings': (3043, 3427),\n",
    "        'transformer_scores': (3427, 3449)\n",
    "    }\n",
    "    print(\"Using known feature structure (3449 features)\")\n",
    "elif n_features == 3451:\n",
    "    # Legacy support for old padded version\n",
    "    feature_categories = {\n",
    "        'medical_patterns': (0, 43),\n",
    "        'word_char_features': (43, 3063),\n",
    "        'sentence_embeddings': (3063, 3449),\n",
    "        'transformer_scores': (3449, 3451)\n",
    "    }\n",
    "    print(\"Using legacy feature structure (3451 features with padding)\")\n",
    "else:\n",
    "    # Generic categorization\n",
    "    print(f\"[WARNING] Unknown feature structure ({n_features} features)\")\n",
    "    print(\"   Skipping category-based analysis\")\n",
    "    feature_categories = None\n",
    "\n",
    "if feature_categories:\n",
    "    for cat_name, (start, end) in feature_categories.items():\n",
    "        n_cat = end - start\n",
    "        print(f\"  {cat_name:25s}: indices {start:4d}-{end-1:4d} ({n_cat:4d} features)\")\n",
    "\n",
    "# ============================================================\n",
    "# CLASS IMBALANCE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "neg = (y_all == 0).sum()\n",
    "pos = (y_all == 1).sum()\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "print(f\"Negative samples: {neg} ({neg/len(y_all)*100:.2f}%)\")\n",
    "print(f\"Positive samples: {pos} ({pos/len(y_all)*100:.2f}%)\")\n",
    "print(f\"Imbalance ratio:  {neg/pos:.2f}:1\")\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best params from Optuna (trial 18)\n",
    "params = {\n",
    "    \"learning_rate\": 0.0054,\n",
    "    \"num_leaves\": 118,\n",
    "    \"max_depth\": 9,\n",
    "    \"min_child_samples\": 211,\n",
    "    \"feature_fraction\": 0.795,\n",
    "    \"bagging_fraction\": 0.813,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l1\": 4.29,\n",
    "    \"lambda_l2\": 4.33,\n",
    "    \"n_estimators\": 4000,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in params.items():\n",
    "    if key not in ['objective', 'metric', 'n_jobs', 'verbosity']:\n",
    "        print(f\"  {key:25s}: {value}\")\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN 5-FOLD ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING 5-FOLD ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = []\n",
    "fold_results = []\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_all, y_all)):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold+1}/5\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
    "    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n",
    "    \n",
    "    print(f\"Train: {X_tr.shape[0]} samples, {y_tr.sum()} positive\")\n",
    "    print(f\"Val:   {X_va.shape[0]} samples, {y_va.sum()} positive\")\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(200, verbose=False),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    models.append(model)\n",
    "    \n",
    "    # Evaluate fold\n",
    "    va_pred_proba = model.predict_proba(X_va)[:,1]\n",
    "    va_auc = roc_auc_score(y_va, va_pred_proba)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'val_auc': va_auc,\n",
    "        'n_iterations': model.best_iteration_\n",
    "    })\n",
    "    \n",
    "    print(f\"Fold {fold+1} Val AUC: {va_auc:.6f}\")\n",
    "    print(f\"Best iteration: {model.best_iteration_}\")\n",
    "\n",
    "# Save fold results\n",
    "fold_df = pd.DataFrame(fold_results)\n",
    "fold_df.to_csv(output_path / 'fold_results.csv', index=False)\n",
    "print(f\"\\n[OK] Fold results saved to: {output_path / 'fold_results.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean Val AUC: {fold_df['val_auc'].mean():.6f} +/- {fold_df['val_auc'].std():.6f}\")\n",
    "print(f\"Mean iterations: {fold_df['n_iterations'].mean():.0f} +/- {fold_df['n_iterations'].std():.0f}\")\n",
    "\n",
    "# ============================================================\n",
    "# ENSEMBLE PREDICTION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def predict_ensemble(X):\n",
    "    \"\"\"Predict using ensemble averaging\"\"\"\n",
    "    preds = np.zeros(X.shape[0])\n",
    "    for m in models:\n",
    "        preds += m.predict_proba(X)[:,1]\n",
    "    return preds / len(models)\n",
    "\n",
    "# ============================================================\n",
    "# OOF PREDICTIONS (NO LEAKAGE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPUTING OUT-OF-FOLD PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "oof_preds = np.zeros(X_all.shape[0])\n",
    "\n",
    "for fold, (_, va_idx) in enumerate(kf.split(X_all, y_all)):\n",
    "    model = models[fold]\n",
    "    oof_preds[va_idx] = model.predict_proba(X_all[va_idx])[:,1]\n",
    "\n",
    "oof_auc = roc_auc_score(y_all, oof_preds)\n",
    "print(f\"OOF ROC-AUC: {oof_auc:.6f}\")\n",
    "\n",
    "# ============================================================\n",
    "# THRESHOLD OPTIMIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find optimal thresholds for different objectives\n",
    "def find_optimal_threshold(y_true, y_pred, metric='f1', min_precision=0.3):\n",
    "    \"\"\"Find optimal threshold for given metric\"\"\"\n",
    "    thresholds = np.linspace(0.01, 0.99, 200)\n",
    "    best_score = -1\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for t in thresholds:\n",
    "        pred = (y_pred >= t).astype(int)\n",
    "        \n",
    "        # Check minimum precision\n",
    "        prec = precision_score(y_true, pred, zero_division=0)\n",
    "        if prec < min_precision:\n",
    "            continue\n",
    "        \n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_true, pred, zero_division=0)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, pred, zero_division=0)\n",
    "        elif metric == 'balanced_accuracy':\n",
    "            score = balanced_accuracy_score(y_true, pred)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = t\n",
    "    \n",
    "    return best_threshold, best_score\n",
    "\n",
    "# Optimize for different objectives\n",
    "objectives = ['f1', 'recall', 'balanced_accuracy']\n",
    "optimal_thresholds = {}\n",
    "\n",
    "print(\"\\nOptimal thresholds (OOF):\")\n",
    "print(f\"{'Objective':<20} {'Threshold':>12} {'Score':>12}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for obj in objectives:\n",
    "    thresh, score = find_optimal_threshold(y_all, oof_preds, metric=obj, min_precision=0.3)\n",
    "    optimal_thresholds[obj] = thresh\n",
    "    print(f\"{obj:<20} {thresh:>12.4f} {score:>12.4f}\")\n",
    "\n",
    "# Use F1-optimized threshold as default\n",
    "best_thr = optimal_thresholds['f1']\n",
    "print(f\"\\n[OK] Using F1-optimized threshold: {best_thr:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Average feature importance across all folds\n",
    "all_importances = []\n",
    "for model in models:\n",
    "    all_importances.append(model.feature_importances_)\n",
    "\n",
    "avg_importance = np.mean(all_importances, axis=0)\n",
    "std_importance = np.std(all_importances, axis=0)\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature_idx': range(len(avg_importance)),\n",
    "    'importance_mean': avg_importance,\n",
    "    'importance_std': std_importance\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "# Add category labels if available\n",
    "if feature_categories:\n",
    "    def get_category(idx):\n",
    "        for cat_name, (start, end) in feature_categories.items():\n",
    "            if start <= idx < end:\n",
    "                return cat_name\n",
    "        return 'unknown'\n",
    "    \n",
    "    importance_df['category'] = importance_df['feature_idx'].apply(get_category)\n",
    "\n",
    "# Top 50 features\n",
    "top_50 = importance_df.head(50)\n",
    "print(\"\\nTop 50 Most Important Features:\")\n",
    "print(top_50.to_string(index=False))\n",
    "\n",
    "# Save full importance\n",
    "importance_df.to_csv(output_path / 'feature_importance_complete.csv', index=False)\n",
    "print(f\"\\n[OK] Saved: {output_path / 'feature_importance_complete.csv'}\")\n",
    "\n",
    "# Category-wise importance\n",
    "if feature_categories:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPORTANCE BY CATEGORY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    category_importance = importance_df.groupby('category').agg({\n",
    "        'importance_mean': ['sum', 'mean', 'count']\n",
    "    }).round(6)\n",
    "    category_importance.columns = ['total_gain', 'avg_gain', 'n_features']\n",
    "    category_importance = category_importance.sort_values('total_gain', ascending=False)\n",
    "    category_importance['total_gain_pct'] = category_importance['total_gain'] / category_importance['total_gain'].sum() * 100\n",
    "    \n",
    "    print(category_importance)\n",
    "    \n",
    "    # Save category importance\n",
    "    category_importance.to_csv(output_path / 'category_importance.csv')\n",
    "    print(f\"\\n[OK] Saved: {output_path / 'category_importance.csv'}\")\n",
    "\n",
    "# ============================================================\n",
    "# ABLATION STUDY\n",
    "# ============================================================\n",
    "\n",
    "if feature_categories:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ABLATION STUDY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ablation_results = []\n",
    "    \n",
    "    # Baseline (all features)\n",
    "    print(\"\\nBaseline (all features):\")\n",
    "    baseline_auc = oof_auc\n",
    "    baseline_pred = (oof_preds >= best_thr).astype(int)\n",
    "    baseline_f1 = f1_score(y_all, baseline_pred)\n",
    "    \n",
    "    print(f\"  ROC-AUC: {baseline_auc:.6f}\")\n",
    "    print(f\"  F1:      {baseline_f1:.6f}\")\n",
    "    \n",
    "    ablation_results.append({\n",
    "        'configuration': 'baseline (all features)',\n",
    "        'n_features': X_all.shape[1],\n",
    "        'removed_category': None,\n",
    "        'auc_mean': baseline_auc,\n",
    "        'auc_std': 0.0,\n",
    "        'f1': baseline_f1,\n",
    "        'auc_delta': 0.0,\n",
    "        'auc_delta_pct': 0.0\n",
    "    })\n",
    "    \n",
    "    # Remove each category\n",
    "    for cat_name, (start, end) in feature_categories.items():\n",
    "        print(f\"\\nRemoving: {cat_name} (features {start}-{end-1})\")\n",
    "        \n",
    "        # Create feature mask\n",
    "        mask = np.ones(X_all.shape[1], dtype=bool)\n",
    "        mask[start:end] = False\n",
    "        \n",
    "        # Get reduced features\n",
    "        X_reduced = X_all[:, mask]\n",
    "        n_features_kept = X_reduced.shape[1]\n",
    "        n_features_removed = end - start\n",
    "        \n",
    "        print(f\"  Keeping {n_features_kept} features, removing {n_features_removed}\")\n",
    "        \n",
    "        # 5-fold CV for ablation (proper methodology)\n",
    "        fold_aucs = []\n",
    "        for fold_idx, (tr_idx, va_idx) in enumerate(kf.split(X_all, y_all)):\n",
    "            X_tr_red = X_reduced[tr_idx]\n",
    "            X_va_red = X_reduced[va_idx]\n",
    "            y_tr = y_all[tr_idx]\n",
    "            y_va = y_all[va_idx]\n",
    "            \n",
    "            # Train model with same params as baseline\n",
    "            ablation_model = lgb.LGBMClassifier(**params)\n",
    "            ablation_model.fit(\n",
    "                X_tr_red, y_tr,\n",
    "                eval_set=[(X_va_red, y_va)],\n",
    "                callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            va_pred = ablation_model.predict_proba(X_va_red)[:,1]\n",
    "            fold_auc = roc_auc_score(y_va, va_pred)\n",
    "            fold_aucs.append(fold_auc)\n",
    "            \n",
    "            print(f\"    Fold {fold_idx+1}: {fold_auc:.6f}\")\n",
    "        \n",
    "        mean_auc = np.mean(fold_aucs)\n",
    "        std_auc = np.std(fold_aucs)\n",
    "        auc_delta = baseline_auc - mean_auc\n",
    "        auc_delta_pct = (auc_delta / baseline_auc) * 100\n",
    "        \n",
    "        print(f\"  Mean AUC: {mean_auc:.6f} +/- {std_auc:.6f}\")\n",
    "        print(f\"  Impact: Delta AUC = {auc_delta:+.6f} ({auc_delta_pct:+.2f}%)\")\n",
    "        \n",
    "        ablation_results.append({\n",
    "            'configuration': f'without {cat_name}',\n",
    "            'n_features': n_features_kept,\n",
    "            'removed_category': cat_name,\n",
    "            'auc_mean': mean_auc,\n",
    "            'auc_std': std_auc,\n",
    "            'f1': np.nan,  # Not computed for speed\n",
    "            'auc_delta': auc_delta,\n",
    "            'auc_delta_pct': auc_delta_pct\n",
    "        })\n",
    "    \n",
    "    # Save ablation results\n",
    "    ablation_df = pd.DataFrame(ablation_results).sort_values('auc_delta', ascending=False)\n",
    "    ablation_df.to_csv(output_path / 'ablation_results.csv', index=False)\n",
    "    print(f\"\\n[OK] Saved: {output_path / 'ablation_results.csv'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ABLATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(ablation_df[['configuration', 'n_features', 'auc_mean', 'auc_std', 'auc_delta_pct']].to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# TOP-K FEATURE ANALYSIS - CORRECTED WITH 5-FOLD CV\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP-K FEATURE EFFICIENCY ANALYSIS (5-FOLD CV)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Using proper cross-validation to ensure valid comparisons\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "topk_results = []\n",
    "k_values = [10, 25, 50, 100, 200, 500, 1000, 2000, 3000]\n",
    "\n",
    "for k in k_values:\n",
    "    if k >= X_all.shape[1]:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing top {k} features with 5-fold CV\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get top k feature indices (by mean importance)\n",
    "    top_k_indices = importance_df.head(k)['feature_idx'].values\n",
    "    X_topk = X_all[:, top_k_indices]\n",
    "    \n",
    "    # 5-fold CV (SAME splits as baseline!)\n",
    "    fold_aucs = []\n",
    "    fold_f1s = []\n",
    "    fold_iterations = []\n",
    "    \n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(kf.split(X_all, y_all)):\n",
    "        X_tr = X_topk[tr_idx]\n",
    "        X_va = X_topk[va_idx]\n",
    "        y_tr = y_all[tr_idx]\n",
    "        y_va = y_all[va_idx]\n",
    "        \n",
    "        # Train with SAME parameters as baseline (critical!)\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        va_pred = model.predict_proba(X_va)[:,1]\n",
    "        fold_auc = roc_auc_score(y_va, va_pred)\n",
    "        \n",
    "        va_pred_binary = (va_pred >= best_thr).astype(int)\n",
    "        fold_f1 = f1_score(y_va, va_pred_binary, zero_division=0)\n",
    "        \n",
    "        fold_aucs.append(fold_auc)\n",
    "        fold_f1s.append(fold_f1)\n",
    "        fold_iterations.append(model.best_iteration_)\n",
    "        \n",
    "        print(f\"  Fold {fold_idx+1}: AUC={fold_auc:.6f}, F1={fold_f1:.6f}, Iter={model.best_iteration_}\")\n",
    "    \n",
    "    mean_auc = np.mean(fold_aucs)\n",
    "    std_auc = np.std(fold_aucs)\n",
    "    mean_f1 = np.mean(fold_f1s)\n",
    "    mean_iter = np.mean(fold_iterations)\n",
    "    \n",
    "    pct_baseline = (mean_auc / baseline_auc) * 100\n",
    "    auc_delta = baseline_auc - mean_auc\n",
    "    \n",
    "    print(f\"\\n  Summary:\")\n",
    "    print(f\"    Mean AUC:       {mean_auc:.6f} +/- {std_auc:.6f}\")\n",
    "    print(f\"    % of baseline:  {pct_baseline:.2f}%\")\n",
    "    print(f\"    Delta AUC:      {auc_delta:+.6f}\")\n",
    "    print(f\"    Mean F1:        {mean_f1:.6f}\")\n",
    "    print(f\"    Mean iterations: {mean_iter:.0f}\")\n",
    "    \n",
    "    topk_results.append({\n",
    "        'k': k,\n",
    "        'auc_mean': mean_auc,\n",
    "        'auc_std': std_auc,\n",
    "        'f1_mean': mean_f1,\n",
    "        'pct_baseline': pct_baseline,\n",
    "        'auc_delta': auc_delta,\n",
    "        'mean_iterations': mean_iter\n",
    "    })\n",
    "\n",
    "# Add baseline (all features) to top-k results\n",
    "topk_results.append({\n",
    "    'k': X_all.shape[1],\n",
    "    'auc_mean': baseline_auc,\n",
    "    'auc_std': fold_df['val_auc'].std(),\n",
    "    'f1_mean': baseline_f1,\n",
    "    'pct_baseline': 100.0,\n",
    "    'auc_delta': 0.0,\n",
    "    'mean_iterations': fold_df['n_iterations'].mean()\n",
    "})\n",
    "\n",
    "# Save top-k results\n",
    "topk_df = pd.DataFrame(topk_results).sort_values('k')\n",
    "topk_df.to_csv(output_path / 'topk_results.csv', index=False)\n",
    "print(f\"\\n[OK] Saved: {output_path / 'topk_results.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP-K SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(topk_df[['k', 'auc_mean', 'auc_std', 'pct_baseline', 'auc_delta']].to_string(index=False))\n",
    "\n",
    "# Verify monotonicity\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MONOTONICITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "sorted_topk = topk_df.sort_values('k')\n",
    "is_monotonic = all(sorted_topk['auc_mean'].iloc[i] <= sorted_topk['auc_mean'].iloc[i+1] \n",
    "                   for i in range(len(sorted_topk)-1))\n",
    "if is_monotonic:\n",
    "    print(\"✓ Results are monotonic (performance increases with more features)\")\n",
    "else:\n",
    "    print(\"⚠ Results are NOT perfectly monotonic (expected due to CV variance)\")\n",
    "    print(\"  This is normal with proper CV - small non-monotonicity is measurement noise\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST SET EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_probs = predict_ensemble(X_test)\n",
    "\n",
    "# Evaluate at different thresholds\n",
    "print(\"\\nPerformance at different thresholds:\")\n",
    "print(f\"{'Threshold':<12} {'Recall':>10} {'Precision':>12} {'F1':>10} {'Detected':>12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "test_results_by_threshold = []\n",
    "\n",
    "for t in [0.5, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, best_thr]:\n",
    "    test_pred = (test_probs >= t).astype(int)\n",
    "    \n",
    "    recall = recall_score(y_test, test_pred, zero_division=0)\n",
    "    precision = precision_score(y_test, test_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, test_pred, zero_division=0)\n",
    "    \n",
    "    n_detected = (test_pred & y_test).sum()\n",
    "    n_total = y_test.sum()\n",
    "    \n",
    "    print(f\"{t:<12.4f} {recall:>10.4f} {precision:>12.4f} {f1:>10.4f} {n_detected:>6}/{n_total:<5}\")\n",
    "    \n",
    "    test_results_by_threshold.append({\n",
    "        'threshold': t,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1,\n",
    "        'n_detected': n_detected,\n",
    "        'n_total': n_total\n",
    "    })\n",
    "\n",
    "# Save threshold analysis\n",
    "threshold_df = pd.DataFrame(test_results_by_threshold)\n",
    "threshold_df.to_csv(output_path / 'test_threshold_analysis.csv', index=False)\n",
    "\n",
    "# Final test evaluation with optimal threshold\n",
    "test_pred_optimal = (test_probs >= best_thr).astype(int)\n",
    "\n",
    "test_auc = roc_auc_score(y_test, test_probs)\n",
    "test_recall = recall_score(y_test, test_pred_optimal)\n",
    "test_precision = precision_score(y_test, test_pred_optimal)\n",
    "test_f1 = f1_score(y_test, test_pred_optimal)\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, test_pred_optimal)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"FINAL TEST RESULTS (threshold={best_thr:.4f})\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ROC-AUC:           {test_auc:.6f}\")\n",
    "print(f\"F1:                {test_f1:.6f}\")\n",
    "print(f\"Precision:         {test_precision:.6f}\")\n",
    "print(f\"Recall:            {test_recall:.6f}\")\n",
    "print(f\"Balanced Accuracy: {test_balanced_acc:.6f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, test_pred_optimal)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {tn:>6}\")\n",
    "print(f\"  False Positives: {fp:>6}\")\n",
    "print(f\"  False Negatives: {fn:>6}\")\n",
    "print(f\"  True Positives:  {tp:>6}\")\n",
    "print(f\"\\nSpecificity: {tn/(tn+fp):.4f}\")\n",
    "print(f\"Sensitivity: {tp/(tp+fn):.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, test_pred_optimal, digits=4))\n",
    "\n",
    "# Save test results\n",
    "test_summary = {\n",
    "    'threshold': best_thr,\n",
    "    'roc_auc': test_auc,\n",
    "    'f1': test_f1,\n",
    "    'precision': test_precision,\n",
    "    'recall': test_recall,\n",
    "    'balanced_accuracy': test_balanced_acc,\n",
    "    'tn': int(tn),\n",
    "    'fp': int(fp),\n",
    "    'fn': int(fn),\n",
    "    'tp': int(tp),\n",
    "    'oof_auc': oof_auc,\n",
    "    'mean_fold_auc': fold_df['val_auc'].mean(),\n",
    "    'std_fold_auc': fold_df['val_auc'].std()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_path / 'test_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n[OK] Saved: {output_path / 'test_results.json'}\")\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION: ROC CURVE & PR CURVE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. ROC Curve\n",
    "ax = axes[0, 0]\n",
    "fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'Test (AUC={test_auc:.4f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "ax = axes[0, 1]\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, test_probs)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "ax.plot(recall_curve, precision_curve, linewidth=2, label=f'PR AUC={pr_auc:.4f}')\n",
    "ax.axhline(y_test.mean(), color='r', linestyle='--', alpha=0.3, label=f'Baseline={y_test.mean():.3f}')\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature Importance (Top 30)\n",
    "ax = axes[1, 0]\n",
    "top_30 = importance_df.head(30)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 30))\n",
    "ax.barh(range(30), top_30['importance_mean'].values, color=colors)\n",
    "ax.set_yticks(range(30))\n",
    "ax.set_yticklabels([f\"Feature {idx}\" for idx in top_30['feature_idx'].values], fontsize=8)\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12)\n",
    "ax.set_title('Top 30 Features by Importance', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Confusion Matrix Heatmap\n",
    "ax = axes[1, 1]\n",
    "cm_display = confusion_matrix(y_test, test_pred_optimal)\n",
    "sns.heatmap(cm_display, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix (threshold={best_thr:.3f})', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'test_evaluation_summary.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"[OK] Saved: {output_path / 'test_evaluation_summary.png'}\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION: CATEGORY IMPORTANCE\n",
    "# ============================================================\n",
    "\n",
    "if feature_categories and 'category' in importance_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Category importance pie chart\n",
    "    ax = axes[0]\n",
    "    category_totals = importance_df.groupby('category')['importance_mean'].sum().sort_values(ascending=False)\n",
    "    colors_pie = plt.cm.Set3(np.linspace(0, 1, len(category_totals)))\n",
    "    wedges, texts, autotexts = ax.pie(category_totals.values, \n",
    "                                        labels=category_totals.index,\n",
    "                                        autopct='%1.1f%%',\n",
    "                                        colors=colors_pie,\n",
    "                                        startangle=90)\n",
    "    ax.set_title('Feature Importance by Category', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Category importance bar chart\n",
    "    ax = axes[1]\n",
    "    ax.bar(range(len(category_totals)), category_totals.values, color=colors_pie)\n",
    "    ax.set_xticks(range(len(category_totals)))\n",
    "    ax.set_xticklabels(category_totals.index, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Total Importance (Gain)', fontsize=12)\n",
    "    ax.set_title('Category Importance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'category_importance.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"[OK] Saved: {output_path / 'category_importance.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION: ABLATION IMPACT\n",
    "# ============================================================\n",
    "\n",
    "if feature_categories and len(ablation_results) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ablation_plot_df = ablation_df[ablation_df['removed_category'].notna()].copy()\n",
    "    \n",
    "    colors_abl = ['red' if x > 0 else 'green' for x in ablation_plot_df['auc_delta']]\n",
    "    \n",
    "    # Add error bars\n",
    "    ax.barh(range(len(ablation_plot_df)), ablation_plot_df['auc_delta'].values, \n",
    "            xerr=ablation_plot_df['auc_std'].values, color=colors_abl, alpha=0.7, capsize=5)\n",
    "    ax.set_yticks(range(len(ablation_plot_df)))\n",
    "    ax.set_yticklabels(ablation_plot_df['removed_category'].values)\n",
    "    ax.set_xlabel('AUC Impact (Baseline - Without Category)', fontsize=12)\n",
    "    ax.set_title('Ablation Study: Impact of Removing Each Category', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'ablation_impact.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"[OK] Saved: {output_path / 'ablation_impact.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION: TOP-K PERFORMANCE WITH ERROR BARS\n",
    "# ============================================================\n",
    "\n",
    "if len(topk_results) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # AUC vs K with error bars\n",
    "    ax = axes[0]\n",
    "    ax.errorbar(topk_df['k'], topk_df['auc_mean'], yerr=topk_df['auc_std'], \n",
    "                marker='o', linewidth=2, capsize=5, label='Top-K (5-fold CV)')\n",
    "    ax.axhline(baseline_auc, color='r', linestyle='--', label='Baseline (all features)')\n",
    "    ax.set_xlabel('Number of Features (K)', fontsize=12)\n",
    "    ax.set_ylabel('ROC-AUC', fontsize=12)\n",
    "    ax.set_title('Performance vs Number of Features', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # % of Baseline vs K\n",
    "    ax = axes[1]\n",
    "    ax.plot(topk_df['k'], topk_df['pct_baseline'], marker='o', linewidth=2, color='green')\n",
    "    ax.axhline(100, color='r', linestyle='--', label='100% baseline')\n",
    "    ax.axhline(99, color='orange', linestyle=':', label='99% baseline')\n",
    "    ax.axhline(95, color='gray', linestyle=':', label='95% baseline', alpha=0.5)\n",
    "    ax.set_xlabel('Number of Features (K)', fontsize=12)\n",
    "    ax.set_ylabel('% of Baseline Performance', fontsize=12)\n",
    "    ax.set_title('Feature Efficiency Analysis (5-Fold CV)', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'topk_performance.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"[OK] Saved: {output_path / 'topk_performance.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION: THRESHOLD ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Recall/Precision vs Threshold\n",
    "ax = axes[0]\n",
    "thresholds_fine = np.linspace(0.05, 0.95, 100)\n",
    "recalls = [recall_score(y_test, (test_probs >= t).astype(int), zero_division=0) for t in thresholds_fine]\n",
    "precisions = [precision_score(y_test, (test_probs >= t).astype(int), zero_division=0) for t in thresholds_fine]\n",
    "\n",
    "ax.plot(thresholds_fine, recalls, label='Recall', linewidth=2)\n",
    "ax.plot(thresholds_fine, precisions, label='Precision', linewidth=2)\n",
    "ax.axvline(best_thr, color='r', linestyle='--', label=f'Optimal ({best_thr:.3f})')\n",
    "ax.set_xlabel('Threshold', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Recall & Precision vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Number detected vs Threshold\n",
    "ax = axes[1]\n",
    "n_detected = [(test_probs >= t).sum() for t in thresholds_fine]\n",
    "ax.plot(thresholds_fine, n_detected, linewidth=2, color='purple')\n",
    "ax.axvline(best_thr, color='r', linestyle='--', label=f'Optimal ({best_thr:.3f})')\n",
    "ax.set_xlabel('Threshold', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples Flagged', fontsize=12)\n",
    "ax.set_title('Detection Volume vs Threshold', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'threshold_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"[OK] Saved: {output_path / 'threshold_analysis.png'}\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# FINAL SUMMARY REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report_lines = [\n",
    "    \"=\"*80,\n",
    "    \"COMPREHENSIVE ANALYSIS REPORT - CORRECTED VERSION\",\n",
    "    \"=\"*80,\n",
    "    \"\",\n",
    "    \"DATASET SUMMARY\",\n",
    "    \"-\"*80,\n",
    "    f\"Train samples:      {len(y_train):>8,}  ({y_train.mean()*100:.2f}% positive)\",\n",
    "    f\"Validation samples: {len(y_val):>8,}  ({y_val.mean()*100:.2f}% positive)\",\n",
    "    f\"Test samples:       {len(y_test):>8,}  ({y_test.mean()*100:.2f}% positive)\",\n",
    "    f\"Total features:     {X_all.shape[1]:>8,}\",\n",
    "    \"\",\n",
    "    \"CROSS-VALIDATION RESULTS\",\n",
    "    \"-\"*80,\n",
    "    f\"OOF AUC:            {oof_auc:.6f}\",\n",
    "    f\"Mean Fold AUC:      {fold_df['val_auc'].mean():.6f} +/- {fold_df['val_auc'].std():.6f}\",\n",
    "    \"\",\n",
    "    \"TEST SET PERFORMANCE\",\n",
    "    \"-\"*80,\n",
    "    f\"Optimal Threshold:  {best_thr:.4f}\",\n",
    "    f\"ROC-AUC:            {test_auc:.6f}\",\n",
    "    f\"F1-Score:           {test_f1:.6f}\",\n",
    "    f\"Precision:          {test_precision:.6f}\",\n",
    "    f\"Recall:             {test_recall:.6f}\",\n",
    "    f\"Balanced Accuracy:  {test_balanced_acc:.6f}\",\n",
    "    \"\",\n",
    "    \"CONFUSION MATRIX\",\n",
    "    \"-\"*80,\n",
    "    f\"True Negatives:     {tn:>8,}\",\n",
    "    f\"False Positives:    {fp:>8,}\",\n",
    "    f\"False Negatives:    {fn:>8,}\",\n",
    "    f\"True Positives:     {tp:>8,}\",\n",
    "    \"\",\n",
    "    f\"Detected:           {tp}/{tp+fn} errors ({tp/(tp+fn)*100:.1f}%)\",\n",
    "    f\"Missed:             {fn}/{tp+fn} errors ({fn/(tp+fn)*100:.1f}%)\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "if feature_categories:\n",
    "    report_lines.extend([\n",
    "        \"FEATURE CATEGORY IMPORTANCE\",\n",
    "        \"-\"*80,\n",
    "    ])\n",
    "    for cat_name in category_importance.index:\n",
    "        total_pct = category_importance.loc[cat_name, 'total_gain_pct']\n",
    "        report_lines.append(f\"{cat_name:25s}: {total_pct:>6.2f}%\")\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "report_lines.extend([\n",
    "    \"TOP-K EFFICIENCY (5-FOLD CV)\",\n",
    "    \"-\"*80,\n",
    "])\n",
    "for _, row in topk_df.iterrows():\n",
    "    report_lines.append(f\"K={row['k']:>5}: {row['auc_mean']:.6f} +/- {row['auc_std']:.6f} ({row['pct_baseline']:>5.2f}%)\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.extend([\n",
    "    \"FILES GENERATED\",\n",
    "    \"-\"*80,\n",
    "    \"[OK] fold_results.csv\",\n",
    "    \"[OK] feature_importance_complete.csv\",\n",
    "    \"[OK] test_results.json\",\n",
    "    \"[OK] test_threshold_analysis.csv\",\n",
    "    \"[OK] test_evaluation_summary.png\",\n",
    "])\n",
    "\n",
    "if feature_categories:\n",
    "    report_lines.extend([\n",
    "        \"[OK] category_importance.csv\",\n",
    "        \"[OK] category_importance.png\",\n",
    "        \"[OK] ablation_results.csv\",\n",
    "        \"[OK] ablation_impact.png\",\n",
    "    ])\n",
    "\n",
    "if len(topk_results) > 0:\n",
    "    report_lines.extend([\n",
    "        \"[OK] topk_results.csv\",\n",
    "        \"[OK] topk_performance.png\",\n",
    "    ])\n",
    "\n",
    "report_lines.extend([\n",
    "    \"[OK] threshold_analysis.png\",\n",
    "    \"\",\n",
    "    \"=\"*80,\n",
    "    \"ANALYSIS COMPLETE\",\n",
    "    \"=\"*80,\n",
    "])\n",
    "\n",
    "report_text = \"\\n\".join(report_lines)\n",
    "print(report_text)\n",
    "\n",
    "# Save report\n",
    "with open(output_path / 'ANALYSIS_REPORT.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(f\"\\n[OK] Full report saved to: {output_path / 'ANALYSIS_REPORT.txt'}\")\n",
    "print(f\"\\n[OK] All outputs saved to: {output_path.absolute()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUCCESS - ALL ANALYSES COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKEY IMPROVEMENTS IN THIS VERSION:\")\n",
    "print(\"  ✓ Top-K analysis uses proper 5-fold CV (not single split)\")\n",
    "print(\"  ✓ Same parameters for all K values (n_estimators=4000)\")\n",
    "print(\"  ✓ Same folds as baseline (fair comparison)\")\n",
    "print(\"  ✓ Error bars included in results and plots\")\n",
    "print(\"  ✓ Results should be monotonic (or nearly so)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
